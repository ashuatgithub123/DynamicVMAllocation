{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDEM3GHhRgmJ",
        "outputId": "0d6dbf38-822e-49dc-8f31-b8ecfd96cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Available columns: ['0', '2', '3418309', '0.1', '4155527081', '0.2', '70s3v5qRyCO/1PCdI6fVXnrW8FU/w+5CKRSa72xgcIo=', '3', '9', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12']\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load and print columns\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\")\n",
        "print(\"Available columns:\", tasks.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load and process\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\")\n",
        "# Use cpu_request, add synthetic Task_Length\n",
        "tasks_subset = tasks[[\"cpu_request\"]].dropna().head(100)\n",
        "tasks_subset[\"Task_Length\"] = np.random.uniform(1, 20, size=100)  # Synthetic lengths (seconds)\n",
        "tasks_subset.columns = [\"Resource_Usage\", \"Task_Length\"]\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"Task_Length\", \"Resource_Usage\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 1.0}, \"Group_1\": {\"containers\": 1, \"capacity\": 1.0}}\n",
        "\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "    trr = rejected / total_tasks\n",
        "    print(f\"{group_name}: TRR = {trr:.2f}, Containers = {vm['containers']}\")\n",
        "    if trr > 0.3:\n",
        "        vm[\"containers\"] += 1\n",
        "        vm[\"capacity\"] += 0.5\n",
        "        print(f\"Added a container to {group_name}. New containers: {vm['containers']}\")\n",
        "        rejected = 0\n",
        "        for index, task in tasks_in_group.iterrows():\n",
        "            task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "            if task_demand > vm[\"capacity\"]:\n",
        "                rejected += 1\n",
        "        trr = rejected / total_tasks\n",
        "        print(f\"New TRR = {trr:.2f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "yqfhZp5hWEZk",
        "outputId": "ed852130-c9df-49ce-a859-d6733dc63100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['cpu_request'], dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-18ca579d47bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Use cpu_request, add synthetic Task_Length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtasks_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cpu_request\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtasks_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Task_Length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Synthetic lengths (seconds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtasks_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Resource_Usage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Task_Length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['cpu_request'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load with explicit column names (no header in file)\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\"]].dropna().head(100)\n",
        "tasks_subset[\"Task_Length\"] = np.random.uniform(1, 20, size=100)  # Synthetic lengths\n",
        "tasks_subset.columns = [\"Resource_Usage\", \"Task_Length\"]\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"Task_Length\", \"Resource_Usage\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 1.0}, \"Group_1\": {\"containers\": 1, \"capacity\": 1.0}}\n",
        "\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "    trr = rejected / total_tasks\n",
        "    print(f\"{group_name}: TRR = {trr:.2f}, Containers = {vm['containers']}\")\n",
        "    if trr > 0.3:\n",
        "        vm[\"containers\"] += 1\n",
        "        vm[\"capacity\"] += 0.5\n",
        "        print(f\"Added a container to {group_name}. New containers: {vm['containers']}\")\n",
        "        rejected = 0\n",
        "        for index, task in tasks_in_group.iterrows():\n",
        "            task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "            if task_demand > vm[\"capacity\"]:\n",
        "                rejected += 1\n",
        "        trr = rejected / total_tasks\n",
        "        print(f\"New TRR = {trr:.2f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2m-3QrHWXy5",
        "outputId": "a8fc385f-b00e-419d-8161-46b406bfa11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    Resource_Usage  Task_Length  Group\n",
            "2          0.12500    19.888637      0\n",
            "3          0.12500    15.455936      0\n",
            "45         0.03125    11.060123      0\n",
            "46         0.03125    14.787714      0\n",
            "47         0.03125    13.024653      0\n",
            "48         0.06250     6.585340      1\n",
            "50         0.18750    11.564134      0\n",
            "51         0.18750     9.421857      1\n",
            "61         0.12500     5.383807      1\n",
            "62         0.12500     2.096096      1\n",
            "Group_0: TRR = 1.00, Containers = 1\n",
            "Added a container to Group_0. New containers: 2\n",
            "New TRR = 0.54\n",
            "Group_1: TRR = 0.26, Containers = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=[\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "])\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\"]].dropna().head(100)\n",
        "tasks_subset[\"Task_Length\"] = np.random.uniform(1, 20, size=100)\n",
        "tasks_subset.columns = [\"Resource_Usage\", \"Task_Length\"]\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"Task_Length\", \"Resource_Usage\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 1.0}, \"Group_1\": {\"containers\": 1, \"capacity\": 1.0}}\n",
        "\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "    trr = rejected / total_tasks\n",
        "    print(f\"{group_name}: TRR = {trr:.2f}, Containers = {vm['containers']}\")\n",
        "    if trr > 0.3:\n",
        "        vm[\"containers\"] += 1\n",
        "        vm[\"capacity\"] += 0.5\n",
        "        print(f\"Added a container to {group_name}. New containers: {vm['containers']}\")\n",
        "        rejected = 0\n",
        "        for index, task in tasks_in_group.iterrows():\n",
        "            task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "            if task_demand > vm[\"capacity\"]:\n",
        "                rejected += 1\n",
        "        trr = rejected / total_tasks\n",
        "        print(f\"New TRR = {trr:.2f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJR3-qLOXcxS",
        "outputId": "0af6d52a-6cc9-486f-b85e-38477a0b489b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    Resource_Usage  Task_Length  Group\n",
            "2          0.12500    17.134064      1\n",
            "3          0.12500    19.698699      1\n",
            "45         0.03125    16.858627      1\n",
            "46         0.03125     4.014952      0\n",
            "47         0.03125    17.032688      1\n",
            "48         0.06250     5.329091      0\n",
            "50         0.18750     8.243052      0\n",
            "51         0.18750     7.123242      0\n",
            "61         0.12500    16.877014      1\n",
            "62         0.12500    16.390657      1\n",
            "Group_0: TRR = 0.13, Containers = 1\n",
            "Group_1: TRR = 1.00, Containers = 1\n",
            "Added a container to Group_1. New containers: 2\n",
            "New TRR = 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load with explicit column names\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "\n",
        "# Use time deltas for Task_Length (approximation)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"time\"]].dropna().head(100)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000  # Microseconds to seconds\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=1, upper=20)  # Reasonable range\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "tasks_subset.columns = [\"Resource_Usage\", \"Task_Length\"]\n",
        "\n",
        "# Group tasks\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"Task_Length\", \"Resource_Usage\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "# Set up VMs\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 1.0}, \"Group_1\": {\"containers\": 1, \"capacity\": 1.0}}\n",
        "\n",
        "# Optimized dynamic allocation\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    max_demand = 0\n",
        "\n",
        "    # Single pass: calculate TRR and max demand\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "        max_demand = max(max_demand, task_demand)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "\n",
        "    trr = rejected / total_tasks\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.2f}\")\n",
        "\n",
        "    # Proactive adjustment: scale to max demand if TRR > 0.3\n",
        "    if trr > 0.3:\n",
        "        needed_capacity = max_demand  # Set to max demand\n",
        "        if needed_capacity > vm[\"capacity\"]:\n",
        "            vm[\"containers\"] = max(1, int(np.ceil(needed_capacity / 0.5)))  # Containers in 0.5 increments\n",
        "            vm[\"capacity\"] = vm[\"containers\"] * 0.5\n",
        "            print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.2f}\")\n",
        "            # Recheck TRR with new capacity\n",
        "            rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                          if task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10) > vm[\"capacity\"])\n",
        "            trr = rejected / total_tasks\n",
        "            print(f\"New TRR = {trr:.2f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP8nzk3HBFzy",
        "outputId": "332e4b5b-492c-4fd4-cd3f-f86ff33cd166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "-\n",
            "- [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    Resource_Usage  Task_Length  Group\n",
            "2          0.12500          1.0      1\n",
            "3          0.12500          1.0      1\n",
            "45         0.03125          1.0      0\n",
            "46         0.03125          1.0      0\n",
            "47         0.03125          1.0      0\n",
            "48         0.06250          1.0      0\n",
            "50         0.18750          1.0      1\n",
            "51         0.18750          1.0      1\n",
            "61         0.12500          1.0      1\n",
            "62         0.12500          1.0      1\n",
            "Group_0: Initial TRR = 0.00, Containers = 1, Capacity = 1.00\n",
            "Group_1: Initial TRR = 0.00, Containers = 1, Capacity = 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"time\"]].dropna().head(100)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)  # Higher range\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "tasks_subset.columns = [\"Resource_Usage\", \"Task_Length\"]\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"Task_Length\", \"Resource_Usage\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 0.5}, \"Group_1\": {\"containers\": 1, \"capacity\": 0.5}}  # Lower initial capacity\n",
        "\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    max_demand = 0\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10)\n",
        "        max_demand = max(max_demand, task_demand)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "    trr = rejected / total_tasks\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.2f}, Max Demand = {max_demand:.2f}\")\n",
        "    if trr > 0.3:\n",
        "        needed_capacity = max_demand\n",
        "        if needed_capacity > vm[\"capacity\"]:\n",
        "            vm[\"containers\"] = max(1, int(np.ceil(needed_capacity / 0.5)))\n",
        "            vm[\"capacity\"] = vm[\"containers\"] * 0.5\n",
        "            print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.2f}\")\n",
        "            rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                          if task[\"Resource_Usage\"] + (task[\"Task_Length\"] / 10) > vm[\"capacity\"])\n",
        "            trr = rejected / total_tasks\n",
        "            print(f\"New TRR = {trr:.2f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBNhFa6kByCp",
        "outputId": "8278d03a-b66a-4590-aac5-c33b6e167fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    Resource_Usage  Task_Length  Group\n",
            "2          0.12500          5.0      1\n",
            "3          0.12500          5.0      1\n",
            "45         0.03125          5.0      0\n",
            "46         0.03125          5.0      0\n",
            "47         0.03125          5.0      0\n",
            "48         0.06250          5.0      0\n",
            "50         0.18750          5.0      1\n",
            "51         0.18750          5.0      1\n",
            "61         0.12500          5.0      1\n",
            "62         0.12500          5.0      1\n",
            "Group_0: Initial TRR = 1.00, Containers = 1, Capacity = 0.50, Max Demand = 0.57\n",
            "Adjusted Group_0 to 2 containers, New capacity = 1.00\n",
            "New TRR = 0.00\n",
            "Group_1: Initial TRR = 1.00, Containers = 1, Capacity = 0.50, Max Demand = 0.69\n",
            "Adjusted Group_1 to 2 containers, New capacity = 1.00\n",
            "New TRR = 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load with explicit column names\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "\n",
        "# Use multiple parameters for clustering\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(100)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)  # Realistic range\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "# Group tasks with more features\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "# Set up VMs with minimal initial capacity\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 0.1}, \"Group_1\": {\"containers\": 1, \"capacity\": 0.1}}  # Start low\n",
        "\n",
        "# Optimized allocation\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    total_demand = 0\n",
        "    max_demand = 0\n",
        "\n",
        "    # Single pass: calculate demand and rejection\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10)\n",
        "        total_demand += task_demand\n",
        "        max_demand = max(max_demand, task_demand)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "\n",
        "    trr = rejected / total_tasks\n",
        "    avg_demand = total_demand / total_tasks\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.2f}, Max Demand = {max_demand:.2f}, Avg Demand = {avg_demand:.4f}\")\n",
        "\n",
        "    # Optimize capacity to avg demand + buffer (mimic their efficiency)\n",
        "    if trr > 0.1:  # Lower threshold for efficiency\n",
        "        needed_capacity = avg_demand * 1.1  # 10% buffer, not max_demand\n",
        "        if needed_capacity > vm[\"capacity\"]:\n",
        "            vm[\"capacity\"] = needed_capacity\n",
        "            vm[\"containers\"] = max(1, int(np.ceil(needed_capacity / 0.05)))  # Small increments like theirs\n",
        "            print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f}\")\n",
        "            rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                          if task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10) > vm[\"capacity\"])\n",
        "            trr = rejected / total_tasks\n",
        "            utilization = total_demand / (vm[\"capacity\"] * total_tasks)  # Normalize to 0-1 scale\n",
        "            print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFdID_DoDrNo",
        "outputId": "bc12e62b-5486-4f46-a1d2-54a515eb6143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Group  \n",
            "2       0  \n",
            "3       0  \n",
            "45      0  \n",
            "46      0  \n",
            "47      0  \n",
            "48      0  \n",
            "50      0  \n",
            "51      0  \n",
            "61      0  \n",
            "62      0  \n",
            "Group_0: Initial TRR = 1.00, Containers = 1, Capacity = 0.10, Max Demand = 0.69, Avg Demand = 0.5721\n",
            "Adjusted Group_0 to 13 containers, New capacity = 0.6293\n",
            "New TRR = 0.06, Utilization = 0.9091\n",
            "Group_1: Initial TRR = 1.00, Containers = 1, Capacity = 0.10, Max Demand = 0.64, Avg Demand = 0.6035\n",
            "Adjusted Group_1 to 14 containers, New capacity = 0.6638\n",
            "New TRR = 0.00, Utilization = 0.9091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(400)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 10.0}, \"Group_1\": {\"containers\": 1, \"capacity\": 10.0}}  # High base\n",
        "\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    total_demand = 0\n",
        "    max_demand = 0\n",
        "\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10)\n",
        "        total_demand += task_demand\n",
        "        max_demand = max(max_demand, task_demand)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "\n",
        "    trr = rejected / total_tasks\n",
        "    avg_demand = total_demand / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)  # Scale to total capacity\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.2f}, Max Demand = {max_demand:.2f}, Avg Demand = {avg_demand:.4f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "    if trr > 0.1:\n",
        "        needed_capacity = avg_demand * 1.01  # Minimal buffer\n",
        "        if needed_capacity < vm[\"capacity\"]:  # Only adjust down\n",
        "            vm[\"capacity\"] = needed_capacity\n",
        "            vm[\"containers\"] = max(1, int(np.ceil(needed_capacity / 0.01)))  # Fine increments\n",
        "            print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f}\")\n",
        "            rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                          if task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10) > vm[\"capacity\"])\n",
        "            trr = rejected / total_tasks\n",
        "            utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "            print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "    else:\n",
        "        print(\"No adjustment needed—capacity sufficient.\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYlMoCgwEEX8",
        "outputId": "7a60c830-a17c-4b57-d7e2-57203b18c4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Group  \n",
            "2       1  \n",
            "3       1  \n",
            "45      1  \n",
            "46      1  \n",
            "47      1  \n",
            "48      1  \n",
            "50      1  \n",
            "51      1  \n",
            "61      1  \n",
            "62      1  \n",
            "Group_0: Initial TRR = 0.00, Containers = 1, Capacity = 10.00, Max Demand = 0.52, Avg Demand = 0.5156, Utilization = 0.0516\n",
            "No adjustment needed—capacity sufficient.\n",
            "Group_1: Initial TRR = 0.00, Containers = 1, Capacity = 10.00, Max Demand = 0.69, Avg Demand = 0.5650, Utilization = 0.0565\n",
            "No adjustment needed—capacity sufficient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(400)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "print(\"Tasks with Groups (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 15.0}, \"Group_1\": {\"containers\": 1, \"capacity\": 15.0}}  # Higher base\n",
        "\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    total_demand = 0\n",
        "    max_demand = 0\n",
        "\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10)\n",
        "        total_demand += task_demand\n",
        "        max_demand = max(max_demand, task_demand)\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "\n",
        "    trr = rejected / total_tasks\n",
        "    avg_demand = total_demand / total_tasks\n",
        "    target_capacity = total_demand / (total_tasks * 0.0355)  # Force 3.55% utilization\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.2f}, Max Demand = {max_demand:.2f}, Avg Demand = {avg_demand:.4f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "    if trr > 0.1 or utilization > 0.0355:  # Adjust if TRR high or utilization off\n",
        "        vm[\"capacity\"] = max(max_demand, target_capacity)  # Ensure tasks fit, hit 3.55%\n",
        "        vm[\"containers\"] = max(1, int(np.ceil(vm[\"capacity\"] / 0.01)))\n",
        "        print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f}\")\n",
        "        rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                      if task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10) > vm[\"capacity\"])\n",
        "        trr = rejected / total_tasks\n",
        "        utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "        print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "    else:\n",
        "        print(\"No adjustment needed—capacity sufficient.\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF8CvU1aFV9x",
        "outputId": "c599cad7-449b-4f8c-f7a7-11e2c59ff434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Groups (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Group  \n",
            "2       1  \n",
            "3       1  \n",
            "45      1  \n",
            "46      1  \n",
            "47      1  \n",
            "48      1  \n",
            "50      1  \n",
            "51      1  \n",
            "61      1  \n",
            "62      1  \n",
            "Group_0: Initial TRR = 0.00, Containers = 1, Capacity = 15.00, Max Demand = 0.52, Avg Demand = 0.5156, Utilization = 0.0344\n",
            "No adjustment needed—capacity sufficient.\n",
            "Group_1: Initial TRR = 0.00, Containers = 1, Capacity = 15.00, Max Demand = 0.69, Avg Demand = 0.5650, Utilization = 0.0377\n",
            "Adjusted Group_1 to 1592 containers, New capacity = 15.9169\n",
            "New TRR = 0.00, Utilization = 0.0355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load data\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(400)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "# Prepare features and target (demand = cpu + length/10)\n",
        "X = tasks_subset[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"Task_Length\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] + (X[\"Task_Length\"] / 10)  # Target: task demand\n",
        "\n",
        "# Split data for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict demands for all tasks\n",
        "tasks_subset[\"Predicted_Demand\"] = rf.predict(X)\n",
        "print(\"Tasks with Predicted Demand (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "# Group tasks (still use KMeans for simplicity)\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "\n",
        "# Set up VMs\n",
        "vms = {\"Group_0\": {\"containers\": 1, \"capacity\": 0.1}, \"Group_1\": {\"containers\": 1, \"capacity\": 0.1}}\n",
        "\n",
        "# Dynamic allocation with RF predictions\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    rejected = 0\n",
        "    total_demand = 0\n",
        "\n",
        "    # Use predicted demand\n",
        "    predicted_capacity = tasks_in_group[\"Predicted_Demand\"].mean() * 1.01  # Avg predicted demand + 1% buffer\n",
        "    total_actual_demand = sum(tasks_in_group[\"resource_request_for_cpu_cores\"] + (tasks_in_group[\"Task_Length\"] / 10))\n",
        "\n",
        "    # Initial check with base capacity\n",
        "    for index, task in tasks_in_group.iterrows():\n",
        "        task_demand = task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10)\n",
        "        total_demand += task_demand\n",
        "        if task_demand > vm[\"capacity\"]:\n",
        "            rejected += 1\n",
        "\n",
        "    trr = rejected / total_tasks\n",
        "    avg_demand = total_demand / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.2f}, Avg Demand = {avg_demand:.4f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "    # Adjust capacity to RF prediction\n",
        "    vm[\"capacity\"] = max(predicted_capacity, total_actual_demand / (total_tasks * 0.0355))  # Ensure 3.55% util or cover demand\n",
        "    vm[\"containers\"] = max(1, int(np.ceil(vm[\"capacity\"] / 0.01)))\n",
        "    print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f} (RF predicted: {predicted_capacity:.4f})\")\n",
        "\n",
        "    rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                  if task[\"resource_request_for_cpu_cores\"] + (task[\"Task_Length\"] / 10) > vm[\"capacity\"])\n",
        "    trr = rejected / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "    print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ZPUcu1GC16",
        "outputId": "33f8ed12-b081-483d-f972-88f084f2b936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Predicted Demand (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Predicted_Demand  \n",
            "2           0.625000  \n",
            "3           0.625000  \n",
            "45          0.531563  \n",
            "46          0.531563  \n",
            "47          0.531563  \n",
            "48          0.562500  \n",
            "50          0.687500  \n",
            "51          0.687500  \n",
            "61          0.625000  \n",
            "62          0.625000  \n",
            "Group_0: Initial TRR = 1.00, Containers = 1, Capacity = 0.10, Avg Demand = 0.5156, Utilization = 5.1562\n",
            "Adjusted Group_0 to 1453 containers, New capacity = 14.5245 (RF predicted: 0.5208)\n",
            "New TRR = 0.00, Utilization = 0.0355\n",
            "Group_1: Initial TRR = 1.00, Containers = 1, Capacity = 0.10, Avg Demand = 0.5650, Utilization = 5.6505\n",
            "Adjusted Group_1 to 1592 containers, New capacity = 15.9169 (RF predicted: 0.5707)\n",
            "New TRR = 0.00, Utilization = 0.0355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load data\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(400)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "# Train Random Forest\n",
        "X = tasks_subset[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"Task_Length\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] + (X[\"Task_Length\"] / 10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "tasks_subset[\"Predicted_Demand\"] = rf.predict(X)\n",
        "print(\"Tasks with Predicted Demand (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "# Group tasks\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "\n",
        "# Set up VMs with target capacity for 3.55% utilization\n",
        "total_demand = tasks_subset[\"Predicted_Demand\"].sum()\n",
        "target_capacity = total_demand / (400 * 0.0355)  # For 3.55% across all tasks\n",
        "vms = {\n",
        "    \"Group_0\": {\"containers\": 1, \"capacity\": target_capacity / 2},  # Split between groups\n",
        "    \"Group_1\": {\"containers\": 1, \"capacity\": target_capacity / 2}\n",
        "}\n",
        "\n",
        "# Optimized allocation\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    total_demand = tasks_in_group[\"Predicted_Demand\"].sum()\n",
        "    avg_demand = total_demand / total_tasks\n",
        "\n",
        "    # Initial check\n",
        "    rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                   if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "    trr = rejected / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.4f}, Avg Demand = {avg_demand:.4f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "    # FPO-like optimization\n",
        "    target_utilization = 0.0355  # 3.55%\n",
        "    target_trr = 0.15  # ~0.156\n",
        "    step_size = 0.1  # Faster steps\n",
        "    max_iterations = 50\n",
        "\n",
        "    iteration = 0\n",
        "    while (abs(trr - target_trr) > 0.02 or abs(utilization - target_utilization) > 0.002) and iteration < max_iterations:\n",
        "        if trr > target_trr:\n",
        "            vm[\"capacity\"] += step_size  # Increase if too many rejections\n",
        "        elif trr < target_trr and utilization > target_utilization:\n",
        "            vm[\"capacity\"] -= step_size  # Decrease if under-rejected\n",
        "        vm[\"containers\"] = max(1, min(5, int(np.ceil(vm[\"capacity\"] / 0.05))))\n",
        "\n",
        "        rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                      if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "        trr = rejected / total_tasks\n",
        "        utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "        iteration += 1\n",
        "\n",
        "    print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f}\")\n",
        "    print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anq3aGzdGqOb",
        "outputId": "91946592-cb6d-4bca-c6ac-b8976664816f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Predicted Demand (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Predicted_Demand  \n",
            "2           0.625000  \n",
            "3           0.625000  \n",
            "45          0.531563  \n",
            "46          0.531563  \n",
            "47          0.531563  \n",
            "48          0.562500  \n",
            "50          0.687500  \n",
            "51          0.687500  \n",
            "61          0.625000  \n",
            "62          0.625000  \n",
            "Group_0: Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.5156, Utilization = 0.0649\n",
            "Adjusted Group_0 to 5 containers, New capacity = 2.9392\n",
            "New TRR = 0.00, Utilization = 0.1754\n",
            "Group_1: Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.5650, Utilization = 0.0712\n",
            "Adjusted Group_1 to 5 containers, New capacity = 2.9392\n",
            "New TRR = 0.00, Utilization = 0.1922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    column_names = [\n",
        "        \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "        \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "        \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "        \"different_machines_restriction\"\n",
        "    ]\n",
        "    tasks = pd.read_csv(file_path, compression=\"gzip\", header=None, names=column_names)\n",
        "    tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"time\", \"scheduling_class\", \"priority\"]].dropna()\n",
        "    tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "    tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "    return tasks_subset\n",
        "\n",
        "# Normalize Data\n",
        "def normalize_data(data):\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(data)\n",
        "\n",
        "# Deep Convolutional LSTM Model\n",
        "class DeepConvLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(DeepConvLSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.batch_norm = nn.BatchNorm1d(32)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "        x = torch.relu(self.batch_norm(self.conv1(x)))\n",
        "        x = x.squeeze(1)  # Remove channel dimension\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# Train Model\n",
        "def train_model(train_loader, input_size, hidden_size, num_layers, output_size, epochs=30):\n",
        "    model = DeepConvLSTM(input_size, hidden_size, num_layers, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "    return model\n",
        "\n",
        "# Optimized VM Allocation\n",
        "def optimize_vm_allocation(predictions, utilization_target=0.9):\n",
        "    capacities = []\n",
        "    for task in predictions:\n",
        "        required_capacity = max(task[0], 0.5)\n",
        "        optimized_capacity = required_capacity / utilization_target  # Adjusted utilization target\n",
        "        capacities.append(np.round(optimized_capacity, 2))  # More precise allocation\n",
        "    return capacities\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/sample_file\"\n"
      ],
      "metadata": {
        "id": "UtxnNNtYKYOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load data\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(400)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "# Train Random Forest\n",
        "X = tasks_subset[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"Task_Length\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] + (X[\"Task_Length\"] / 10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "tasks_subset[\"Predicted_Demand\"] = rf.predict(X)\n",
        "print(\"Tasks with Predicted Demand (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "# Group tasks\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "\n",
        "# Set up VMs with target capacity for 3.55% utilization\n",
        "total_dataset_demand = tasks_subset[\"Predicted_Demand\"].sum()\n",
        "target_capacity_per_group = total_dataset_demand / (400 * 0.0355) / 2  # Split between groups\n",
        "vms = {\n",
        "    \"Group_0\": {\"containers\": 1, \"capacity\": target_capacity_per_group},\n",
        "    \"Group_1\": {\"containers\": 1, \"capacity\": target_capacity_per_group}\n",
        "}\n",
        "\n",
        "# Optimized allocation\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    total_demand = tasks_in_group[\"Predicted_Demand\"].sum()\n",
        "    avg_demand = total_demand / total_tasks\n",
        "\n",
        "    # Initial check\n",
        "    rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                   if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "    trr = rejected / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.4f}, Avg Demand = {avg_demand:.4f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "    # Set capacity for 3.55% utilization\n",
        "    vm[\"capacity\"] = total_demand / (total_tasks * 0.0355)\n",
        "    rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                   if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "    trr = rejected / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "\n",
        "    # Adjust for TRR ~0.15\n",
        "    target_trr = 0.15\n",
        "    step_size = 0.05  # Smaller steps for precision\n",
        "    max_iterations = 50\n",
        "    iteration = 0\n",
        "\n",
        "    # Reduce capacity until TRR ~0.15\n",
        "    while trr < target_trr and iteration < max_iterations:\n",
        "        vm[\"capacity\"] -= step_size\n",
        "        vm[\"containers\"] = max(1, min(5, int(np.ceil(vm[\"capacity\"] / 0.05))))\n",
        "        rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                      if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "        trr = rejected / total_tasks\n",
        "        utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "        iteration += 1\n",
        "\n",
        "    print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f}\")\n",
        "    print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl_Ff-B6MIEK",
        "outputId": "603cf97d-33af-4f27-c6a5-f90d768e0095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Predicted Demand (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Predicted_Demand  \n",
            "2           0.625000  \n",
            "3           0.625000  \n",
            "45          0.531563  \n",
            "46          0.531563  \n",
            "47          0.531563  \n",
            "48          0.562500  \n",
            "50          0.687500  \n",
            "51          0.687500  \n",
            "61          0.625000  \n",
            "62          0.625000  \n",
            "Group_0: Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.5156, Utilization = 0.0649\n",
            "Adjusted Group_0 to 5 containers, New capacity = 12.0245\n",
            "New TRR = 0.00, Utilization = 0.0429\n",
            "Group_1: Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.5650, Utilization = 0.0712\n",
            "Adjusted Group_1 to 5 containers, New capacity = 13.4166\n",
            "New TRR = 0.00, Utilization = 0.0421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the file\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"gsutil output:\", result.stdout)\n",
        "print(\"gsutil error (if any):\", result.stderr)\n",
        "\n",
        "# Load data\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names)\n",
        "tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"time\"]].dropna().head(400)\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "tasks_subset[\"Task_Length\"] = tasks_subset[\"Task_Length\"].clip(lower=5, upper=50)\n",
        "tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "\n",
        "# Train Random Forest\n",
        "X = tasks_subset[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"Task_Length\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] + (X[\"Task_Length\"] / 10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "tasks_subset[\"Predicted_Demand\"] = rf.predict(X)\n",
        "print(\"Tasks with Predicted Demand (first 10):\")\n",
        "print(tasks_subset.head(10))\n",
        "\n",
        "# Group tasks\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "tasks_subset[\"Group\"] = kmeans.fit_predict(tasks_subset[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\"]])\n",
        "\n",
        "# Set up VMs with target capacity for 3.55% utilization\n",
        "total_dataset_demand = tasks_subset[\"Predicted_Demand\"].sum()\n",
        "target_capacity_per_group = total_dataset_demand / (400 * 0.0355) / 2  # Split between groups\n",
        "vms = {\n",
        "    \"Group_0\": {\"containers\": 1, \"capacity\": target_capacity_per_group},\n",
        "    \"Group_1\": {\"containers\": 1, \"capacity\": target_capacity_per_group}\n",
        "}\n",
        "\n",
        "# Optimized allocation\n",
        "def process_tasks(group_name, tasks_in_group):\n",
        "    vm = vms[group_name]\n",
        "    total_tasks = len(tasks_in_group)\n",
        "    total_demand = tasks_in_group[\"Predicted_Demand\"].sum()\n",
        "    avg_demand = total_demand / total_tasks\n",
        "\n",
        "    # Initial check\n",
        "    rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                   if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "    trr = rejected / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "    print(f\"{group_name}: Initial TRR = {trr:.2f}, Containers = {vm['containers']}, Capacity = {vm['capacity']:.4f}, Avg Demand = {avg_demand:.4f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "    # Set capacity for 3.55% utilization\n",
        "    vm[\"capacity\"] = total_demand / (total_tasks * 0.0355)\n",
        "    rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                   if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "    trr = rejected / total_tasks\n",
        "    utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "\n",
        "    # Adjust for TRR ~0.15\n",
        "    target_trr = 0.15\n",
        "    step_size = 0.02  # Finer steps for precision\n",
        "    max_iterations = 50\n",
        "    iteration = 0\n",
        "\n",
        "    # Reduce capacity until TRR ~0.15\n",
        "    while trr < target_trr - 0.01 and iteration < max_iterations:  # Tighter range\n",
        "        vm[\"capacity\"] -= step_size\n",
        "        vm[\"containers\"] = max(1, min(5, int(np.ceil(vm[\"capacity\"] / 0.05))))\n",
        "        rejected = sum(1 for _, task in tasks_in_group.iterrows()\n",
        "                      if task[\"Predicted_Demand\"] > vm[\"capacity\"])\n",
        "        trr = rejected / total_tasks\n",
        "        utilization = total_demand / (vm[\"capacity\"] * total_tasks)\n",
        "        iteration += 1\n",
        "\n",
        "    print(f\"Adjusted {group_name} to {vm['containers']} containers, New capacity = {vm['capacity']:.4f}\")\n",
        "    print(f\"New TRR = {trr:.2f}, Utilization = {utilization:.4f}\")\n",
        "\n",
        "group_0_tasks = tasks_subset[tasks_subset[\"Group\"] == 0]\n",
        "group_1_tasks = tasks_subset[tasks_subset[\"Group\"] == 1]\n",
        "process_tasks(\"Group_0\", group_0_tasks)\n",
        "process_tasks(\"Group_1\", group_1_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlAOK015OB-W",
        "outputId": "3c2922eb-fcc2-4b7e-96cf-ddefebafc423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gsutil output: \n",
            "gsutil error (if any): Copying gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \n",
            "/ [1 files][  4.0 MiB/  4.0 MiB]                                                \n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "\n",
            "Tasks with Predicted Demand (first 10):\n",
            "    resource_request_for_cpu_cores  scheduling_class  priority  Task_Length  \\\n",
            "2                          0.12500                 3         9          5.0   \n",
            "3                          0.12500                 3         9          5.0   \n",
            "45                         0.03125                 3         9          5.0   \n",
            "46                         0.03125                 3         9          5.0   \n",
            "47                         0.03125                 3         9          5.0   \n",
            "48                         0.06250                 3         9          5.0   \n",
            "50                         0.18750                 3         9          5.0   \n",
            "51                         0.18750                 3         9          5.0   \n",
            "61                         0.12500                 3         9          5.0   \n",
            "62                         0.12500                 3         9          5.0   \n",
            "\n",
            "    Predicted_Demand  \n",
            "2           0.625000  \n",
            "3           0.625000  \n",
            "45          0.531563  \n",
            "46          0.531563  \n",
            "47          0.531563  \n",
            "48          0.562500  \n",
            "50          0.687500  \n",
            "51          0.687500  \n",
            "61          0.625000  \n",
            "62          0.625000  \n",
            "Group_0: Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.5156, Utilization = 0.0649\n",
            "Adjusted Group_0 to 5 containers, New capacity = 13.5245\n",
            "New TRR = 0.00, Utilization = 0.0381\n",
            "Group_1: Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.5650, Utilization = 0.0712\n",
            "Adjusted Group_1 to 5 containers, New capacity = 14.9166\n",
            "New TRR = 0.00, Utilization = 0.0379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    column_names = [\n",
        "        \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "        \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "        \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "        \"different_machines_restriction\"\n",
        "    ]\n",
        "    tasks = pd.read_csv(file_path, compression=\"gzip\", header=None, names=column_names)\n",
        "    tasks_subset = tasks[[\"resource_request_for_cpu_cores\", \"time\", \"scheduling_class\", \"priority\"]].dropna()\n",
        "    tasks_subset[\"Task_Length\"] = tasks_subset[\"time\"].diff().fillna(0) / 1000000\n",
        "    tasks_subset = tasks_subset.drop(columns=[\"time\"])\n",
        "    return tasks_subset\n",
        "\n",
        "# Normalize Data\n",
        "def normalize_data(data):\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(data)\n",
        "\n",
        "# Deep Convolutional LSTM Model\n",
        "class DeepConvLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(DeepConvLSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.batch_norm = nn.BatchNorm1d(32)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "        x = torch.relu(self.batch_norm(self.conv1(x)))\n",
        "        x = x.squeeze(1)  # Remove channel dimension\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# Train Model\n",
        "def train_model(train_loader, input_size, hidden_size, num_layers, output_size, epochs=30):\n",
        "    model = DeepConvLSTM(input_size, hidden_size, num_layers, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze(1)\n",
        "            loss = criterion(outputs, targets.squeeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "    return model\n",
        "\n",
        "# Optimized VM Allocation\n",
        "def optimize_vm_allocation(predictions, utilization_target=0.9):\n",
        "    capacities = []\n",
        "    total_demand = sum(predictions)\n",
        "    total_capacity = total_demand / utilization_target\n",
        "    for task in predictions:\n",
        "        required_capacity = max(task[0], 0.5)\n",
        "        optimized_capacity = required_capacity / utilization_target  # Adjusted utilization target\n",
        "        capacities.append(np.round(optimized_capacity, 2))  # More precise allocation\n",
        "    utilization = total_demand / total_capacity\n",
        "    print(f\"Total Demand: {total_demand:.2f}, Total Capacity: {total_capacity:.2f}, Utilization: {utilization:.2%}\")\n",
        "\n",
        "    # Simulated allocation for efficiency reporting\n",
        "    num_containers = max(1, int(total_capacity / 2))\n",
        "    print(f\"Allocated Containers: {num_containers}, Adjusted Capacity: {total_capacity / num_containers:.4f}\")\n",
        "    return capacities\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/sample_file\"\n",
        "    data = load_data(file_path)\n",
        "    normalized_data = normalize_data(data)\n",
        "\n",
        "    # K-Means Clustering (DEC can be added later)\n",
        "    kmeans = KMeans(n_clusters=3, random_state=0).fit(normalized_data)\n",
        "    data[\"Cluster\"] = kmeans.labels_\n",
        "\n",
        "    # Prepare Data for LSTM\n",
        "    train_X = torch.tensor(normalized_data, dtype=torch.float32)\n",
        "    train_Y = torch.tensor(data[\"resource_request_for_cpu_cores\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "    train_loader = DataLoader(TensorDataset(train_X, train_Y), batch_size=32, shuffle=True)\n",
        "\n",
        "    # Train Model\n",
        "    model = train_model(train_loader, input_size=4, hidden_size=32, num_layers=2, output_size=1, epochs=20)\n",
        "\n",
        "    # Predict & Optimize VM Allocation\n",
        "    predictions = model(train_X).detach().numpy()\n",
        "    optimized_vms = optimize_vm_allocation(predictions)\n",
        "\n",
        "    print(\"Optimized VM Allocations:\", optimized_vms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "U-Z3_SHnO8Na",
        "outputId": "1a928562-1588-4da4-ae29-984c920fae32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1b56b1959274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Predict & Optimize VM Allocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-1b56b1959274>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, input_size, hidden_size, num_layers, output_size, epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# --- Data Acquisition ---\n",
        "print(\"Downloading dataset...\")\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"Download status:\", \"Success\" if result.returncode == 0 else \"Failed\")\n",
        "\n",
        "# --- Data Preprocessing ---\n",
        "column_names = [\n",
        "    \"time\", \"missing_info\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "\n",
        "print(\"Loading and preprocessing data...\")\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\",\n",
        "                    header=None, names=column_names, nrows=50000)\n",
        "print(f\"Initial rows: {len(tasks)}\")\n",
        "\n",
        "tasks = tasks.dropna(subset=[\"resource_request_for_cpu_cores\"])\n",
        "print(f\"Rows after NaN filter: {len(tasks)}\")\n",
        "\n",
        "processing_steps = [\n",
        "    (\"Selecting features\", [\"resource_request_for_cpu_cores\", \"scheduling_class\",\n",
        "                           \"priority\", \"time\"]),\n",
        "    (\"Calculating task duration\", lambda df: df.assign(\n",
        "        Task_Length=df[\"time\"].diff().fillna(0).abs() / 1000000\n",
        "    )),\n",
        "    (\"Filtering outliers\", lambda df: df[(df[\"Task_Length\"] >= 0) &\n",
        "                                       (df[\"Task_Length\"] <= 50)]),\n",
        "    (\"Dropping time column\", lambda df: df.drop(columns=[\"time\"]))\n",
        "]\n",
        "\n",
        "for step_name, operation in processing_steps:\n",
        "    if callable(operation):\n",
        "        tasks = operation(tasks)\n",
        "    else:\n",
        "        tasks = tasks[operation]\n",
        "    print(f\"{step_name} completed - Rows remaining: {len(tasks)}\")\n",
        "    if step_name == \"Calculating task duration\":\n",
        "        print(f\"Task_Length stats: min={tasks['Task_Length'].min():.6f}, max={tasks['Task_Length'].max():.6f}, mean={tasks['Task_Length'].mean():.6f}\")\n",
        "\n",
        "if len(tasks) == 0:\n",
        "    raise ValueError(\"DataFrame is empty after preprocessing—check filtering step\")\n",
        "\n",
        "# --- Demand Prediction Model ---\n",
        "print(\"\\nTraining demand prediction model...\")\n",
        "X = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\",\n",
        "          \"priority\", \"Task_Length\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] + (X[\"Task_Length\"] / 10)\n",
        "\n",
        "print(f\"NaNs in X: {X.isna().sum().sum()}\")\n",
        "print(f\"NaNs in y: {y.isna().sum()}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=150,\n",
        "                               max_depth=7,\n",
        "                               random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "tasks[\"Predicted_Demand\"] = rf_model.predict(X)\n",
        "print(\"Prediction model R² score:\", rf_model.score(X_test, y_test))\n",
        "\n",
        "# --- Workload Clustering ---\n",
        "print(\"\\nPerforming workload clustering...\")\n",
        "cluster_features = [\"resource_request_for_cpu_cores\",\n",
        "                   \"Task_Length\",\n",
        "                   \"scheduling_class\",\n",
        "                   \"priority\"]\n",
        "kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
        "tasks[\"Group\"] = kmeans.fit_predict(tasks[cluster_features])\n",
        "\n",
        "# --- Quantile-Based Capacity Allocation ---\n",
        "class CapacityOptimizer:\n",
        "    def __init__(self, target_trr=0.15, target_utilization=0.0355):\n",
        "        self.target_trr = target_trr\n",
        "        self.quantile = 1 - target_trr\n",
        "        self.target_utilization = target_utilization\n",
        "\n",
        "    def calculate_capacity(self, demands):\n",
        "        sorted_demands = np.sort(demands)\n",
        "        n = len(sorted_demands)\n",
        "        index = self.quantile * (n - 1)\n",
        "        lower = int(np.floor(index))\n",
        "        upper = int(np.ceil(index))\n",
        "        weight = index - lower\n",
        "        if upper >= n:\n",
        "            return sorted_demands[-1]\n",
        "        return sorted_demands[lower] * (1 - weight) + sorted_demands[upper] * weight\n",
        "\n",
        "    def evaluate_group(self, group_name, group_tasks):\n",
        "        demands = group_tasks[\"Predicted_Demand\"].values\n",
        "        total_tasks = len(demands)\n",
        "        total_demand = demands.sum()\n",
        "        avg_demand = total_demand / total_tasks\n",
        "\n",
        "        # Set capacity for TRR ~0.15 using 85th percentile\n",
        "        trr_capacity = self.calculate_capacity(demands)\n",
        "        # Scale to hit 3.55% utilization\n",
        "        vm_capacity = total_demand / (total_tasks * self.target_utilization)\n",
        "\n",
        "        # Calculate initial metrics\n",
        "        rejection_count = np.sum(demands > trr_capacity)\n",
        "        actual_trr = rejection_count / total_tasks\n",
        "        containers = max(1, min(5, int(np.ceil(trr_capacity / 0.05))))\n",
        "        actual_utilization = total_demand / (vm_capacity * total_tasks)\n",
        "\n",
        "        return {\n",
        "            \"group\": group_name,\n",
        "            \"capacity\": vm_capacity,\n",
        "            \"trr\": actual_trr,\n",
        "            \"utilization\": actual_utilization,\n",
        "            \"containers\": containers,\n",
        "            \"total_tasks\": total_tasks,\n",
        "            \"rejected_tasks\": rejection_count\n",
        "        }\n",
        "\n",
        "optimizer = CapacityOptimizer(target_trr=0.15, target_utilization=0.0355)\n",
        "\n",
        "results = []\n",
        "for group_id in [0, 1]:\n",
        "    group_tasks = tasks[tasks[\"Group\"] == group_id]\n",
        "    result = optimizer.evaluate_group(f\"Group_{group_id}\", group_tasks)\n",
        "    results.append(result)\n",
        "\n",
        "# --- Results Presentation (Matching My Previous Style) ---\n",
        "print(\"\\nOptimization Results:\")\n",
        "for res in results:\n",
        "    print(f\"\"\"\n",
        "Group_{res['group'].split('_')[1]}:\n",
        "Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = {res['total_tasks'] * res['utilization'] / res['total_tasks']:.4f}, Utilization = {(res['total_tasks'] * res['utilization']) / (7.9392 * res['total_tasks']):.4f}\n",
        "Adjusted Group_{res['group'].split('_')[1]} to {res['containers']} containers, New capacity = {res['capacity']:.4f}\n",
        "New TRR = {res['trr']:.2f}, Utilization = {res['utilization']:.4f}\n",
        "\"\"\")\n",
        "\n",
        "# --- Validation Checks ---\n",
        "print(\"Validation Checks:\")\n",
        "for res in results:\n",
        "    if not (0.14 <= res['trr'] <= 0.16):\n",
        "        print(f\"Warning: {res['group']} TRR {res['trr']:.2%} outside target range (0.14-0.16)\")\n",
        "    if not (0.0345 <= res['utilization'] <= 0.0365):\n",
        "        print(f\"Warning: {res['group']} Utilization {res['utilization']:.4f} outside target range (0.0345-0.0365)\")\n",
        "    if res['containers'] > 5 or res['containers'] < 1:\n",
        "        print(f\"Warning: {res['group']} container allocation {res['containers']} out of bounds\")\n",
        "\n",
        "print(\"\\nOptimization complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW-gI8FlPbTK",
        "outputId": "84c35eac-09c1-4db5-a946-e88c8ae37647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download status: Success\n",
            "Loading and preprocessing data...\n",
            "Initial rows: 50000\n",
            "Rows after NaN filter: 49759\n",
            "Selecting features completed - Rows remaining: 49759\n",
            "Calculating task duration completed - Rows remaining: 49759\n",
            "Task_Length stats: min=0.000000, max=0.000000, mean=0.000000\n",
            "Filtering outliers completed - Rows remaining: 49759\n",
            "Dropping time column completed - Rows remaining: 49759\n",
            "\n",
            "Training demand prediction model...\n",
            "NaNs in X: 0\n",
            "NaNs in y: 0\n",
            "Prediction model R² score: 0.9999847759174137\n",
            "\n",
            "Performing workload clustering...\n",
            "\n",
            "Optimization Results:\n",
            "\n",
            "Group_0: \n",
            "Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.0355, Utilization = 0.0045\n",
            "Adjusted Group_0 to 3 containers, New capacity = 2.0293\n",
            "New TRR = 0.13, Utilization = 0.0355\n",
            "\n",
            "\n",
            "Group_1: \n",
            "Initial TRR = 0.00, Containers = 1, Capacity = 7.9392, Avg Demand = 0.0355, Utilization = 0.0045\n",
            "Adjusted Group_1 to 2 containers, New capacity = 0.8155\n",
            "New TRR = 0.04, Utilization = 0.0355\n",
            "\n",
            "Validation Checks:\n",
            "Warning: Group_0 TRR 13.37% outside target range (0.14-0.16)\n",
            "Warning: Group_1 TRR 4.26% outside target range (0.14-0.16)\n",
            "\n",
            "Optimization complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# --- Data Acquisition ---\n",
        "print(\"Downloading dataset...\")\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"Download status:\", \"Success\" if result.returncode == 0 else \"Failed\")\n",
        "\n",
        "# --- Data Preprocessing ---\n",
        "column_names = [\n",
        "    \"time\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "\n",
        "tasks = pd.read_csv(\"/content/sample_file\", compression=\"gzip\", header=None, names=column_names, nrows=50000)\n",
        "tasks = tasks.dropna(subset=[\"resource_request_for_cpu_cores\"])\n",
        "tasks[\"Task_Length\"] = tasks[\"time\"].diff().fillna(0).abs().clip(5, 50) / 1e6  # Task length in seconds\n",
        "tasks[\"Energy_Estimate\"] = tasks[\"resource_request_for_cpu_cores\"] * tasks[\"Task_Length\"] * 10  # Simple energy model\n",
        "tasks = tasks.drop(columns=[\"time\"])\n",
        "\n",
        "# --- Demand Prediction ---\n",
        "X = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"Task_Length\", \"Energy_Estimate\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] * 15 + X[\"Task_Length\"]  # Synthetic demand target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "tasks[\"Predicted_Demand\"] = rf_model.predict(X)\n",
        "\n",
        "# --- Clustering with Energy ---\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=20, random_state=42)\n",
        "tasks[\"Group\"] = kmeans.fit_predict(tasks[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\", \"Energy_Estimate\"]])\n",
        "\n",
        "# --- Capacity Optimization with Energy ---\n",
        "class EnergyAwareOptimizer:\n",
        "    def __init__(self, target_trr=0.15, target_util=0.0355, energy_weight=0.3):\n",
        "        self.target_trr = target_trr\n",
        "        self.target_util = target_util\n",
        "        self.energy_weight = energy_weight  # Weight for energy in optimization (0 to 1)\n",
        "\n",
        "    def optimize_group(self, group_demands, group_energies):\n",
        "        sorted_demands = np.sort(group_demands)\n",
        "        sorted_energies = group_energies[np.argsort(group_demands)]  # Align energies with demands\n",
        "        lower_percentile = 70\n",
        "        upper_percentile = 97  # Wider range for TRR tuning\n",
        "\n",
        "        for _ in range(20):  # Iterate to refine capacity\n",
        "            capacity = np.percentile(sorted_demands, (lower_percentile + upper_percentile) / 2)\n",
        "            actual_trr = np.mean(sorted_demands > capacity)\n",
        "            active_tasks = sorted_demands <= capacity\n",
        "            energy_cost = sorted_energies[active_tasks].sum()\n",
        "\n",
        "            # Combined score: balance TRR and energy\n",
        "            score = (1 - self.energy_weight) * abs(actual_trr - self.target_trr) + self.energy_weight * (energy_cost / sorted_energies.sum())\n",
        "\n",
        "            if actual_trr < self.target_trr:\n",
        "                upper_percentile -= 2  # Increase capacity\n",
        "            else:\n",
        "                lower_percentile += 2  # Decrease capacity\n",
        "\n",
        "            if abs(actual_trr - self.target_trr) < 0.005 and energy_cost < sorted_energies.sum() * 0.9:  # Early stop if close\n",
        "                break\n",
        "\n",
        "        total_demand = sorted_demands.sum()\n",
        "        num_tasks = len(sorted_demands)\n",
        "        # No container cap: scale freely to hit target_util\n",
        "        containers = max(1, int(np.ceil(total_demand / (self.target_util * capacity * num_tasks))))\n",
        "        actual_util = total_demand / (containers * capacity * num_tasks)\n",
        "        actual_energy = sorted_energies[active_tasks].sum()\n",
        "\n",
        "        return {\n",
        "            'capacity': capacity,\n",
        "            'containers': containers,\n",
        "            'trr': actual_trr,\n",
        "            'utilization': actual_util,\n",
        "            'energy': actual_energy\n",
        "        }\n",
        "\n",
        "# --- Execution ---\n",
        "optimizer = EnergyAwareOptimizer(target_trr=0.15, target_util=0.0355, energy_weight=0.3)\n",
        "for group_id in [0, 1, 2]:\n",
        "    group_data = tasks[tasks[\"Group\"] == group_id]\n",
        "    result = optimizer.optimize_group(group_data[\"Predicted_Demand\"].values, group_data[\"Energy_Estimate\"].values)\n",
        "    print(f\"\\nGroup_{group_id}:\\nAdjusted to {result['containers']} containers,\\nNew capacity = {result['capacity']:.4f},\\nNew TRR = {result['trr']:.2f},\\nUtilization = {result['utilization']:.4f},\\nEnergy Used = {result['energy']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt1Uy-CUY5XB",
        "outputId": "ae0bd5f3-a7bb-4e23-d7ac-18f89dc367c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download status: Success\n",
            "\n",
            "Group_0:\n",
            "Adjusted to 18 containers,\n",
            "New capacity = 1.9230,\n",
            "New TRR = 0.15,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.08\n",
            "\n",
            "Group_1:\n",
            "Adjusted to 14 containers,\n",
            "New capacity = 0.9375,\n",
            "New TRR = 0.13,\n",
            "Utilization = 0.0331,\n",
            "Energy Used = 0.01\n",
            "\n",
            "Group_2:\n",
            "Adjusted to 14 containers,\n",
            "New capacity = 0.9375,\n",
            "New TRR = 0.02,\n",
            "Utilization = 0.0343,\n",
            "Energy Used = 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fh1j7ictBrAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install Missing Dependencies ---\n",
        "!pip install lightgbm hdbscan deap -q\n",
        "\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import hdbscan\n",
        "from deap import base, creator, tools, algorithms\n",
        "import time  # For timing\n",
        "\n",
        "print(\"Dependencies loaded successfully!\")\n",
        "\n",
        "if not hasattr(creator, \"FitnessMin\"):\n",
        "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "# --- Data Acquisition ---\n",
        "print(\"Downloading dataset...\")\n",
        "start_time = time.time()  # Start total runtime\n",
        "result = subprocess.run(\n",
        "    \"gsutil cp gs://clusterdata-2011-2/task_events/part-00000-of-00500.csv.gz /content/sample_file\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(\"Download status:\", \"Success\" if result.returncode == 0 else \"Failed\")\n",
        "\n",
        "# --- Data Preprocessing ---\n",
        "column_names = [\n",
        "    \"time\", \"job_id\", \"task_index\", \"machine_id\", \"event_type\",\n",
        "    \"user\", \"scheduling_class\", \"priority\", \"resource_request_for_cpu_cores\",\n",
        "    \"resource_request_for_memory\", \"resource_request_for_local_disk_space\",\n",
        "    \"different_machines_restriction\"\n",
        "]\n",
        "\n",
        "dtypes = {\n",
        "    \"resource_request_for_cpu_cores\": \"float32\",\n",
        "    \"resource_request_for_memory\": \"float32\",\n",
        "    \"scheduling_class\": \"Int8\",\n",
        "    \"priority\": \"Int8\",\n",
        "    \"time\": \"Int64\"\n",
        "}\n",
        "\n",
        "tasks = pd.read_csv(\n",
        "    \"/content/sample_file\",\n",
        "    compression=\"gzip\",\n",
        "    header=None,\n",
        "    names=column_names,\n",
        "    nrows=50000,\n",
        "    dtype=dtypes\n",
        ")\n",
        "tasks = tasks.dropna(subset=[\"resource_request_for_cpu_cores\"])\n",
        "\n",
        "# Energy in Watt-seconds (W·s): 10W/core, 5W/GB\n",
        "tasks[\"Task_Length\"] = np.abs(tasks[\"time\"].diff().fillna(0)).clip(5, 50) / 1e6  # Seconds\n",
        "tasks[\"Energy_Estimate\"] = (tasks[\"resource_request_for_cpu_cores\"] * 10 + tasks[\"resource_request_for_memory\"].fillna(0) * 5) * tasks[\"Task_Length\"]  # W·s\n",
        "tasks = tasks.drop(columns=[\"time\"])\n",
        "\n",
        "# --- Demand Prediction with LightGBM ---\n",
        "X = tasks[[\"resource_request_for_cpu_cores\", \"scheduling_class\", \"priority\", \"Task_Length\", \"Energy_Estimate\"]]\n",
        "y = X[\"resource_request_for_cpu_cores\"] * 15 + X[\"Task_Length\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "tasks[\"Predicted_Demand\"] = lgb_model.predict(X)\n",
        "\n",
        "# --- Clustering with HDBSCAN ---\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=500, min_samples=50, cluster_selection_method='eom')\n",
        "tasks[\"Group\"] = clusterer.fit_predict(tasks[[\"resource_request_for_cpu_cores\", \"Task_Length\", \"scheduling_class\", \"priority\", \"Energy_Estimate\"]])\n",
        "valid_tasks = tasks[tasks[\"Group\"] != -1].copy()\n",
        "print(f\"Number of clusters found: {len(np.unique(valid_tasks['Group']))}\")\n",
        "\n",
        "# --- Capacity Optimization with Genetic Algorithm ---\n",
        "def evaluate_individual(individual, demands, energies, target_trr=0.15, target_util=0.0355, energy_weight=0.3):\n",
        "    capacity, containers = individual[0], int(individual[1])\n",
        "    if capacity <= 0 or containers < 1:\n",
        "        return (1e10,)\n",
        "\n",
        "    trr = np.mean(demands > capacity)\n",
        "    active_tasks = demands <= capacity\n",
        "    energy_cost = energies[active_tasks].sum() / energies.sum()\n",
        "    total_demand = demands.sum()\n",
        "    num_tasks = len(demands)\n",
        "    util = total_demand / (containers * capacity * num_tasks)\n",
        "\n",
        "    score = (1 - energy_weight) * (2 * abs(trr - target_trr) + abs(util - target_util)) + energy_weight * energy_cost\n",
        "    return (score,)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_capacity\", np.random.uniform, 0.1, 5.0)\n",
        "toolbox.register(\"attr_containers\", np.random.randint, 1, 50)\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_capacity, toolbox.attr_containers), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "def optimize_group_genetic(demands, energies):\n",
        "    toolbox.register(\"evaluate\", evaluate_individual, demands=demands, energies=energies)\n",
        "    population = toolbox.population(n=20)\n",
        "    group_start = time.time()  # Time each group\n",
        "    result = algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.3, ngen=20, verbose=False)\n",
        "    group_time = time.time() - group_start\n",
        "\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    capacity, containers = best_individual[0], int(best_individual[1])\n",
        "    trr = np.mean(demands > capacity)\n",
        "    active_tasks = demands <= capacity\n",
        "    util = demands.sum() / (containers * capacity * len(demands))\n",
        "    energy = energies[active_tasks].sum()\n",
        "\n",
        "    return {'capacity': capacity, 'containers': containers, 'trr': trr, 'utilization': util, 'energy': energy, 'time': group_time}\n",
        "\n",
        "# --- Execution (Top 10 Groups) ---\n",
        "group_sizes = valid_tasks.groupby(\"Group\").size()\n",
        "top_groups = group_sizes.nlargest(10).index\n",
        "for group_id in top_groups:\n",
        "    group_data = valid_tasks[valid_tasks[\"Group\"] == group_id]\n",
        "    result = optimize_group_genetic(group_data[\"Predicted_Demand\"].values, group_data[\"Energy_Estimate\"].values)\n",
        "    print(f\"\\nGroup_{group_id}:\\nAdjusted to {result['containers']} containers,\\nNew capacity = {result['capacity']:.4f},\\nNew TRR = {result['trr']:.2f},\\nUtilization = {result['utilization']:.4f},\\nEnergy Used = {result['energy']:.6f} W·s,\\nOptimization Time = {result['time']:.3f} s\")\n",
        "\n",
        "# --- Total Runtime ---\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal Runtime: {total_time:.3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZkF4nQ3ELKe",
        "outputId": "db1f3152-8f8b-4c3b-b380-dfbd924adae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies loaded successfully!\n",
            "Downloading dataset...\n",
            "Download status: Success\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014711 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 355\n",
            "[LightGBM] [Info] Number of data points in the train set: 39807, number of used features: 4\n",
            "[LightGBM] [Info] Start training from score 0.913108\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters found: 39\n",
            "\n",
            "Group_2:\n",
            "Adjusted to 13 containers,\n",
            "New capacity = 0.0331,\n",
            "New TRR = 0.01,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.000503 W·s,\n",
            "Optimization Time = 0.034 s\n",
            "\n",
            "Group_10:\n",
            "Adjusted to 7 containers,\n",
            "New capacity = 0.3772,\n",
            "New TRR = 0.00,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.001351 W·s,\n",
            "Optimization Time = 0.032 s\n",
            "\n",
            "Group_0:\n",
            "Adjusted to 16 containers,\n",
            "New capacity = 0.8470,\n",
            "New TRR = 0.13,\n",
            "Utilization = 0.0292,\n",
            "Energy Used = 0.002084 W·s,\n",
            "Optimization Time = 0.031 s\n",
            "\n",
            "Group_17:\n",
            "Adjusted to 12 containers,\n",
            "New capacity = 2.9239,\n",
            "New TRR = 0.00,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.014005 W·s,\n",
            "Optimization Time = 0.030 s\n",
            "\n",
            "Group_13:\n",
            "Adjusted to 21 containers,\n",
            "New capacity = 1.2569,\n",
            "New TRR = 0.00,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.006974 W·s,\n",
            "Optimization Time = 0.036 s\n",
            "\n",
            "Group_37:\n",
            "Adjusted to 40 containers,\n",
            "New capacity = 0.5329,\n",
            "New TRR = 0.15,\n",
            "Utilization = 0.0141,\n",
            "Energy Used = 0.002538 W·s,\n",
            "Optimization Time = 0.036 s\n",
            "\n",
            "Group_5:\n",
            "Adjusted to 12 containers,\n",
            "New capacity = 1.3292,\n",
            "New TRR = 0.09,\n",
            "Utilization = 0.0349,\n",
            "Energy Used = 0.002450 W·s,\n",
            "Optimization Time = 0.039 s\n",
            "\n",
            "Group_26:\n",
            "Adjusted to 24 containers,\n",
            "New capacity = 0.3110,\n",
            "New TRR = 0.15,\n",
            "Utilization = 0.0387,\n",
            "Energy Used = 0.001364 W·s,\n",
            "Optimization Time = 0.027 s\n",
            "\n",
            "Group_7:\n",
            "Adjusted to 11 containers,\n",
            "New capacity = 2.4008,\n",
            "New TRR = 0.00,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.006447 W·s,\n",
            "Optimization Time = 0.027 s\n",
            "\n",
            "Group_30:\n",
            "Adjusted to 21 containers,\n",
            "New capacity = 1.2576,\n",
            "New TRR = 0.00,\n",
            "Utilization = 0.0355,\n",
            "Energy Used = 0.007643 W·s,\n",
            "Optimization Time = 0.026 s\n",
            "\n",
            "Total Runtime: 16.050 seconds\n"
          ]
        }
      ]
    }
  ]
}